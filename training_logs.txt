early_stop_patience :  5.0
Epoch 1/300
1057/1057 [==============================] - 60s 57ms/step - loss: 1.6752 - acc: 0.5683 - val_loss: 0.7655 - val_acc: 0.8438
Epoch 2/300
1057/1057 [==============================] - 54s 51ms/step - loss: 0.4341 - acc: 0.9267 - val_loss: 0.4470 - val_acc: 0.9195
Epoch 3/300
1057/1057 [==============================] - 55s 52ms/step - loss: 0.2764 - acc: 0.9628 - val_loss: 0.3439 - val_acc: 0.9421
Epoch 4/300
1057/1057 [==============================] - 56s 53ms/step - loss: 0.2325 - acc: 0.9727 - val_loss: 0.3540 - val_acc: 0.9384
Epoch 5/300
1057/1057 [==============================] - 57s 54ms/step - loss: 0.2069 - acc: 0.9775 - val_loss: 0.4904 - val_acc: 0.9295
Epoch 6/300
1056/1057 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9811
Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004999999888241291.
1057/1057 [==============================] - 78s 74ms/step - loss: 0.1875 - acc: 0.9811 - val_loss: 0.4519 - val_acc: 0.9285
Epoch 7/300
1057/1057 [==============================] - 58s 55ms/step - loss: 0.1167 - acc: 0.9944 - val_loss: 0.2020 - val_acc: 0.9629
Epoch 8/300
1057/1057 [==============================] - 59s 56ms/step - loss: 0.0660 - acc: 0.9981 - val_loss: 0.1823 - val_acc: 0.9627
Epoch 9/300
1057/1057 [==============================] - 60s 56ms/step - loss: 0.0535 - acc: 0.9990 - val_loss: 0.1799 - val_acc: 0.9626
Epoch 10/300
1055/1057 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9991
Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.9999996554106475e-05.
1057/1057 [==============================] - 60s 56ms/step - loss: 0.0470 - acc: 0.9991 - val_loss: 0.1717 - val_acc: 0.9641
Epoch 11/300
1057/1057 [==============================] - 59s 56ms/step - loss: 0.0425 - acc: 0.9993 - val_loss: 0.1631 - val_acc: 0.9647
Epoch 12/300
1057/1057 [==============================] - 60s 57ms/step - loss: 0.0408 - acc: 0.9993 - val_loss: 0.1595 - val_acc: 0.9653
Epoch 13/300
1055/1057 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9994
Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.999999509891496e-06.
1057/1057 [==============================] - 60s 57ms/step - loss: 0.0399 - acc: 0.9994 - val_loss: 0.1604 - val_acc: 0.9648
Epoch 14/300
1057/1057 [==============================] - 62s 59ms/step - loss: 0.0391 - acc: 0.9995 - val_loss: 0.1599 - val_acc: 0.9649
Epoch 15/300
1057/1057 [==============================] - 62s 58ms/step - loss: 0.0389 - acc: 0.9993 - val_loss: 0.1602 - val_acc: 0.9647
Epoch 16/300
1056/1057 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9996
Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.999999418942025e-07.
1057/1057 [==============================] - 61s 58ms/step - loss: 0.0390 - acc: 0.9996 - val_loss: 0.1602 - val_acc: 0.9648
Epoch 17/300
1057/1057 [==============================] - 63s 60ms/step - loss: 0.0386 - acc: 0.9995 - val_loss: 0.1591 - val_acc: 0.9648
Epoch 18/300
1057/1057 [==============================] - 63s 59ms/step - loss: 0.0385 - acc: 0.9995 - val_loss: 0.1592 - val_acc: 0.9648
Epoch 19/300
1055/1057 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9996
Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.999999418942025e-08.
1057/1057 [==============================] - 59s 56ms/step - loss: 0.0385 - acc: 0.9996 - val_loss: 0.1606 - val_acc: 0.9647
Epoch 20/300
1057/1057 [==============================] - 61s 58ms/step - loss: 0.0389 - acc: 0.9995 - val_loss: 0.1607 - val_acc: 0.9647
Epoch 21/300
1057/1057 [==============================] - 63s 60ms/step - loss: 0.0386 - acc: 0.9997 - val_loss: 0.1605 - val_acc: 0.9646
Epoch 22/300
1056/1057 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9996
Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-08.
1057/1057 [==============================] - 60s 57ms/step - loss: 0.0385 - acc: 0.9996 - val_loss: 0.1610 - val_acc: 0.9644
Epoch 00022: early stopping



_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
block1_conv1 (Conv2D)        (None, 30, 30, 64)        640
_________________________________________________________________
batch_normalization (BatchNo (None, 30, 30, 64)        256
_________________________________________________________________
activation (Activation)      (None, 30, 30, 64)        0
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 28, 28, 64)        36928
_________________________________________________________________
batch_normalization_1 (Batch (None, 28, 28, 64)        256
_________________________________________________________________
activation_1 (Activation)    (None, 28, 28, 64)        0
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 12, 12, 128)       73856
_________________________________________________________________
batch_normalization_2 (Batch (None, 12, 12, 128)       512
_________________________________________________________________
activation_2 (Activation)    (None, 12, 12, 128)       0
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 10, 10, 128)       147584
_________________________________________________________________
batch_normalization_3 (Batch (None, 10, 10, 128)       512
_________________________________________________________________
activation_3 (Activation)    (None, 10, 10, 128)       0
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 128)         0
_________________________________________________________________
flatten (Flatten)            (None, 3200)              0
_________________________________________________________________
fc1 (Dense)                  (None, 356)               1139556
_________________________________________________________________
batch_normalization_4 (Batch (None, 356)               1424
_________________________________________________________________
activation_4 (Activation)    (None, 356)               0
_________________________________________________________________
dropout (Dropout)            (None, 356)               0
_________________________________________________________________
fc2 (Dense)                  (None, 356)               127092
_________________________________________________________________
batch_normalization_5 (Batch (None, 356)               1424
_________________________________________________________________
activation_5 (Activation)    (None, 356)               0
_________________________________________________________________
dropout_1 (Dropout)          (None, 356)               0
_________________________________________________________________
fc3 (Dense)                  (None, 43)                15351
=================================================================
Total params: 1,545,391
Trainable params: 1,543,199
Non-trainable params: 2,192
_________________________________________________________________



Saved model weights to disk
Training history saved
